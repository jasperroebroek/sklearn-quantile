

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Prediction Intervals for Quantile Regression Forests &mdash; sklearn_quantile 0.1.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=a58bc63e"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="What’s new" href="../whats_new.html" />
    <link rel="prev" title="Example usage" href="example_usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            sklearn_quantile
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../methods.html">Quantile Regression Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="../methods.html#quantile-knn">Quantile KNN</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="example_usage.html">Example usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Prediction Intervals for Quantile Regression Forests</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#fitting-non-linear-quantile-and-least-squares-regressors">Fitting non-linear quantile and least squares regressors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#analysis-of-the-error-metrics">Analysis of the error metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calibration-of-the-confidence-interval">Calibration of the confidence interval</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tuning-the-hyper-parameters-of-the-quantile-regressors">Tuning the hyper-parameters of the quantile regressors</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../whats_new.html">What’s new</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sklearn_quantile</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Prediction Intervals for Quantile Regression Forests</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/example_qrf.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>None</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial was generated from an IPython notebook that can be
downloaded <a class="reference external" href="../../../source/notebooks/example_qrf.ipynb">here</a>.</p>
</div>
<section id="prediction-intervals-for-quantile-regression-forests">
<span id="example-qrf"></span><h1>Prediction Intervals for Quantile Regression Forests<a class="headerlink" href="#prediction-intervals-for-quantile-regression-forests" title="Link to this heading"></a></h1>
<p>This example shows how quantile regression can be used to create
prediction intervals. Note that this is an adapted example from Gradient
Boosting regression with quantile loss. The procedure and conclusions
remain almost exactly the same.</p>
<p>Generate some data for a synthetic regression problem by applying the
function f to uniformly sampled random inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The function to predict.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">expected_y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>
</div>
<p>To make the problem interesting, we generate observations of the target
y as the sum of a deterministic term computed by the function f and a
random noise term that follows a centered
<code class="docutils literal notranslate"><span class="pre">log-normal</span> <span class="pre">&lt;https://en.wikipedia.org/wiki/Log-normal_distribution&gt;</span></code>_.
To make this even more interesting we consider the case where the
amplitude of the noise depends on the input variable x (heteroscedastic
noise).</p>
<p>The lognormal distribution is non-symmetric and long tailed: observing
large outliers is likely but it is impossible to observe small outliers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">/</span> <span class="mi">10</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">expected_y</span> <span class="o">+</span> <span class="n">noise</span>
</pre></div>
</div>
<p>Split into train, test datasets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<section id="fitting-non-linear-quantile-and-least-squares-regressors">
<h2>Fitting non-linear quantile and least squares regressors<a class="headerlink" href="#fitting-non-linear-quantile-and-least-squares-regressors" title="Link to this heading"></a></h2>
<p>Fit a Random Forest Regressor and Quantile Regression Forest based on
the same parameterisation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn_quantile</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestQuantileRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_pinball_loss</span><span class="p">,</span> <span class="n">mean_squared_error</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">common_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">qrf</span> <span class="o">=</span> <span class="n">RandomForestQuantileRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">common_params</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">qrf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RandomForestQuantileRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                              <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
</pre></div>
</div>
<p>For the sake of comparison, also fit a standard Regression Forest</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">common_params</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Create an evenly spaced evaluation set of input values spanning the [0,
10] range.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<p>All quantile predictions are done simultaneously.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
</pre></div>
</div>
<p>Plot the true conditional mean function f, the prediction of the
conditional mean (least squares loss), the conditional median and the
conditional 90% interval (from 5th to 95th conditional percentiles).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
<span class="n">y_lower</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_med</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_upper</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="s1">&#39;g:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x\,\sin(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test observations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_med</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted median&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_lower</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_lower</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted 90</span><span class="si">% i</span><span class="s1">nterval&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">dl</span><span class="o">/</span><span class="n">b3rz1nb55sqgldl8hnzqvz0m0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_38586</span><span class="o">/</span><span class="mf">1410534679.</span><span class="n">py</span><span class="p">:</span><span class="mi">11</span><span class="p">:</span> <span class="ne">UserWarning</span><span class="p">:</span> <span class="n">color</span> <span class="ow">is</span> <span class="n">redundantly</span> <span class="n">defined</span> <span class="n">by</span> <span class="n">the</span> <span class="s1">&#39;color&#39;</span> <span class="n">keyword</span> <span class="n">argument</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">fmt</span> <span class="n">string</span> <span class="s2">&quot;r-&quot;</span> <span class="p">(</span><span class="o">-&gt;</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span> <span class="n">The</span> <span class="n">keyword</span> <span class="n">argument</span> <span class="n">will</span> <span class="n">take</span> <span class="n">precedence</span><span class="o">.</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_med</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted median&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/example_qrf_20_1.png" src="../_images/example_qrf_20_1.png" />
<p>Comparing the predicted median with the predicted mean, we note that the
median is on average below the mean as the noise is skewed towards high
values (large outliers). The median estimate also seems to be smoother
because of its natural robustness to outliers.</p>
</section>
<section id="analysis-of-the-error-metrics">
<h2>Analysis of the error metrics<a class="headerlink" href="#analysis-of-the-error-metrics" title="Link to this heading"></a></h2>
<p>Measure the models with :func:<code class="docutils literal notranslate"><span class="pre">sklearn.mean_squared_error</span></code> and
:func:<code class="docutils literal notranslate"><span class="pre">sklearn.mean_pinball_loss</span></code> metrics on the training dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="k">def</span><span class="w"> </span><span class="nf">highlight_min</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_min</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;font-weight: bold&#39;</span> <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="n">x_min</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>


<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;q 0.05&quot;</span><span class="p">,</span> <span class="s2">&quot;q 0.5&quot;</span><span class="p">,</span> <span class="s2">&quot;q 0.95&quot;</span><span class="p">,</span> <span class="s2">&quot;rf&quot;</span><span class="p">]):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;rf&quot;</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]:</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;pbl=</span><span class="si">%1.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">alpha</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_pinball_loss</span><span class="p">(</span>
            <span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;MSE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">highlight_min</span><span class="p">)</span>
</pre></div>
</div>
<style type="text/css">
#T_a596e_row0_col0, #T_a596e_row1_col1, #T_a596e_row2_col2, #T_a596e_row3_col3 {
  font-weight: bold;
}
</style>
<table id="T_a596e">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_a596e_level0_col0" class="col_heading level0 col0" >pbl=0.05</th>
      <th id="T_a596e_level0_col1" class="col_heading level0 col1" >pbl=0.50</th>
      <th id="T_a596e_level0_col2" class="col_heading level0 col2" >pbl=0.95</th>
      <th id="T_a596e_level0_col3" class="col_heading level0 col3" >MSE</th>
    </tr>
    <tr>
      <th class="index_name level0" >model</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
      <th class="blank col2" >&nbsp;</th>
      <th class="blank col3" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_a596e_level0_row0" class="row_heading level0 row0" >q 0.05</th>
      <td id="T_a596e_row0_col0" class="data row0 col0" >0.155918</td>
      <td id="T_a596e_row0_col1" class="data row0 col1" >1.524603</td>
      <td id="T_a596e_row0_col2" class="data row0 col2" >2.893288</td>
      <td id="T_a596e_row0_col3" class="data row0 col3" >21.433560</td>
    </tr>
    <tr>
      <th id="T_a596e_level0_row1" class="row_heading level0 row1" >q 0.5</th>
      <td id="T_a596e_row1_col0" class="data row1 col0" >0.571557</td>
      <td id="T_a596e_row1_col1" class="data row1 col1" >0.752413</td>
      <td id="T_a596e_row1_col2" class="data row1 col2" >0.933269</td>
      <td id="T_a596e_row1_col3" class="data row1 col3" >9.864386</td>
    </tr>
    <tr>
      <th id="T_a596e_level0_row2" class="row_heading level0 row2" >q 0.95</th>
      <td id="T_a596e_row2_col0" class="data row2 col0" >4.252400</td>
      <td id="T_a596e_row2_col1" class="data row2 col1" >2.314031</td>
      <td id="T_a596e_row2_col2" class="data row2 col2" >0.375662</td>
      <td id="T_a596e_row2_col3" class="data row2 col3" >38.857734</td>
    </tr>
    <tr>
      <th id="T_a596e_level0_row3" class="row_heading level0 row3" >rf</th>
      <td id="T_a596e_row3_col0" class="data row3 col0" >0.783801</td>
      <td id="T_a596e_row3_col1" class="data row3 col1" >0.789724</td>
      <td id="T_a596e_row3_col2" class="data row3 col2" >0.795647</td>
      <td id="T_a596e_row3_col3" class="data row3 col3" >9.478518</td>
    </tr>
  </tbody>
</table><p>One column shows all models evaluated by the same metric. The minimum
number on a column should be obtained when the model is trained and
measured with the same metric. This should be always the case on the
training set if the training converged.</p>
<p>Note that because the target distribution is asymmetric, the expected
conditional mean and conditional median are signficiantly different and
therefore one could not use the least squares model get a good
estimation of the conditional median nor the converse.</p>
<p>If the target distribution were symmetric and had no outliers (e.g. with
a Gaussian noise), then median estimator and the least squares estimator
would have yielded similar predictions.</p>
<p>We then do the same on the test set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;q 0.05&quot;</span><span class="p">,</span> <span class="s2">&quot;q 0.5&quot;</span><span class="p">,</span> <span class="s2">&quot;q 0.95&quot;</span><span class="p">,</span> <span class="s2">&quot;rf&quot;</span><span class="p">]):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">model</span> <span class="o">==</span> <span class="s2">&quot;rf&quot;</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">]:</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;pbl=</span><span class="si">%1.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">alpha</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_pinball_loss</span><span class="p">(</span>
            <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;MSE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">highlight_min</span><span class="p">)</span>
</pre></div>
</div>
<style type="text/css">
#T_0fb54_row0_col0, #T_0fb54_row1_col1, #T_0fb54_row1_col3, #T_0fb54_row2_col2 {
  font-weight: bold;
}
</style>
<table id="T_0fb54">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_0fb54_level0_col0" class="col_heading level0 col0" >pbl=0.05</th>
      <th id="T_0fb54_level0_col1" class="col_heading level0 col1" >pbl=0.50</th>
      <th id="T_0fb54_level0_col2" class="col_heading level0 col2" >pbl=0.95</th>
      <th id="T_0fb54_level0_col3" class="col_heading level0 col3" >MSE</th>
    </tr>
    <tr>
      <th class="index_name level0" >model</th>
      <th class="blank col0" >&nbsp;</th>
      <th class="blank col1" >&nbsp;</th>
      <th class="blank col2" >&nbsp;</th>
      <th class="blank col3" >&nbsp;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_0fb54_level0_row0" class="row_heading level0 row0" >q 0.05</th>
      <td id="T_0fb54_row0_col0" class="data row0 col0" >0.152957</td>
      <td id="T_0fb54_row0_col1" class="data row0 col1" >1.457100</td>
      <td id="T_0fb54_row0_col2" class="data row0 col2" >2.761244</td>
      <td id="T_0fb54_row0_col3" class="data row0 col3" >16.976903</td>
    </tr>
    <tr>
      <th id="T_0fb54_level0_row1" class="row_heading level0 row1" >q 0.5</th>
      <td id="T_0fb54_row1_col0" class="data row1 col0" >0.705819</td>
      <td id="T_0fb54_row1_col1" class="data row1 col1" >0.738051</td>
      <td id="T_0fb54_row1_col2" class="data row1 col2" >0.770283</td>
      <td id="T_0fb54_row1_col3" class="data row1 col3" >6.705497</td>
    </tr>
    <tr>
      <th id="T_0fb54_level0_row2" class="row_heading level0 row2" >q 0.95</th>
      <td id="T_0fb54_row2_col0" class="data row2 col0" >5.131204</td>
      <td id="T_0fb54_row2_col1" class="data row2 col1" >2.763838</td>
      <td id="T_0fb54_row2_col2" class="data row2 col2" >0.396472</td>
      <td id="T_0fb54_row2_col3" class="data row2 col3" >67.215948</td>
    </tr>
    <tr>
      <th id="T_0fb54_level0_row3" class="row_heading level0 row3" >rf</th>
      <td id="T_0fb54_row3_col0" class="data row3 col0" >0.957845</td>
      <td id="T_0fb54_row3_col1" class="data row3 col1" >0.809511</td>
      <td id="T_0fb54_row3_col2" class="data row3 col2" >0.661178</td>
      <td id="T_0fb54_row3_col3" class="data row3 col3" >7.094214</td>
    </tr>
  </tbody>
</table><p>Errors are very similar to the ones for the training data, meaning that
the model is fitting reasonably well on the data.</p>
<p>Note that the conditional median estimator is actually showing a lower
MSE in comparison to the standard Regression Forests: this can be
explained by the fact the least squares estimator is very sensitive to
large outliers which can cause significant overfitting. This can be seen
on the right hand side of the previous plot. The conditional median
estimator is biased (underestimation for this asymetric noise) but is
also naturally robust to outliers and overfits less.</p>
</section>
<section id="calibration-of-the-confidence-interval">
<h2>Calibration of the confidence interval<a class="headerlink" href="#calibration-of-the-confidence-interval" title="Link to this heading"></a></h2>
<p>We can also evaluate the ability of the two extreme quantile estimators at
producing a well-calibrated conditational 90%-confidence interval.</p>
<p>To do this we can compute the fraction of observations that fall between the
predictions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">coverage_fraction</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_low</span><span class="p">,</span> <span class="n">y_high</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">y</span> <span class="o">&gt;=</span> <span class="n">y_low</span><span class="p">,</span> <span class="n">y</span> <span class="o">&lt;=</span> <span class="n">y_high</span><span class="p">))</span>


<span class="n">coverage_fraction</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span>
                  <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                  <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.9293333333333333</span>
</pre></div>
</div>
<p>On the training set the calibration is very close to the expected
coverage value for a 90% confidence interval.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">coverage_fraction</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
                  <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                  <span class="n">qrf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.916</span>
</pre></div>
</div>
<p>On the test set the coverage is even closer to the expected 90%.</p>
</section>
<section id="tuning-the-hyper-parameters-of-the-quantile-regressors">
<h2>Tuning the hyper-parameters of the quantile regressors<a class="headerlink" href="#tuning-the-hyper-parameters-of-the-quantile-regressors" title="Link to this heading"></a></h2>
<p>In the plot above, we observed that the 5th percentile predictions seems
to underfit and could not adapt to sinusoidal shape of the signal.</p>
<p>The hyper-parameters of the model were approximately hand-tuned for the
median regressor and there is no reason than the same hyper-parameters
are suitable for the 5th percentile regressor.</p>
<p>To confirm this hypothesis, we tune the hyper-parameters of each
quantile separately with the pinball loss with alpha being the quantile
of the regressor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>


<span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="n">min_samples_split</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">neg_mean_pinball_loss_05p_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span>
    <span class="n">mean_pinball_loss</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># maximize the negative loss</span>
<span class="p">)</span>
<span class="n">qrf</span> <span class="o">=</span> <span class="n">RandomForestQuantileRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
<span class="n">search_05p</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">qrf</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># increase this if computational budget allows</span>
    <span class="n">scoring</span><span class="o">=</span><span class="n">neg_mean_pinball_loss_05p_scorer</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">search_05p</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
 <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
 <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">}</span>
</pre></div>
</div>
<p>We observe that the search procedure identifies that deeper trees are
needed to get a good fit for the 5th percentile regressor. Deeper trees
are more expressive and less likely to underfit.</p>
<p>Let’s now tune the hyper-parameters for the 95th percentile regressor.
We need to redefine the <code class="docutils literal notranslate"><span class="pre">scoring</span></code> metric used to select the best
model, along with adjusting the quantile parameter of the inner gradient
boosting estimator itself:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">clone</span>

<span class="n">q</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">neg_mean_pinball_loss_95p_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span>
    <span class="n">mean_pinball_loss</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># maximize the negative loss</span>
<span class="p">)</span>
<span class="n">search_95p</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">search_05p</span><span class="p">)</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
    <span class="n">estimator__q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="n">neg_mean_pinball_loss_95p_scorer</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">search_95p</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">search_95p</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
 <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">150</span><span class="p">}</span>
</pre></div>
</div>
<p>This time, shallower trees are selected and lead to a more constant
piecewise and therefore more robust estimation of the 95th percentile.
This is beneficial as it avoids overfitting the large outliers of the
log-normal additive noise.</p>
<p>We can confirm this intuition by displaying the predicted 90% confidence
interval comprised by the predictions of those two tuned quantile
regressors: the prediction of the upper 95th percentile has a much
coarser shape than the prediction of the lower 5th percentile:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_lower</span> <span class="o">=</span> <span class="n">search_05p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
<span class="n">y_upper</span> <span class="o">=</span> <span class="n">search_95p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xx</span><span class="p">),</span> <span class="s1">&#39;g:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x\,\sin(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test observations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">y_lower</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">y_lower</span><span class="p">,</span> <span class="n">y_upper</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted 90</span><span class="si">% i</span><span class="s1">nterval&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prediction with tuned hyper-parameters&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/example_qrf_40_0.png" src="../_images/example_qrf_40_0.png" />
<p>The plot looks qualitatively better than for the untuned models,
especially for the shape of the of lower quantile.</p>
<p>We now quantitatively evaluate the joint-calibration of the pair of
estimators:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">coverage_fraction</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span>
                  <span class="n">search_05p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
                  <span class="n">search_95p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.94</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">coverage_fraction</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
                  <span class="n">search_05p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
                  <span class="n">search_95p</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.9</span>
</pre></div>
</div>
<p>The calibrated pinball loss on the test set is exactly the expected 90
percent coverage.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="example_usage.html" class="btn btn-neutral float-left" title="Example usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../whats_new.html" class="btn btn-neutral float-right" title="What’s new" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Jasper Roebroek.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>